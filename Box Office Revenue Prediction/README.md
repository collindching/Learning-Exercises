This is an exercise to reinforce what I've learned in chapter 2 of Hands-On ML. I'll be building a regression model for box office revenues using movie data from The Movie Database. 

One of the key things I wanted to practice in this project was how to process data using custom transformers and Scikit-Learn Pipelines. This actually ended up taking a pretty long time to test code. The data preprocessing step actually felt more like programming and coding than machine learning, and I spent a lot of time debugging. 

This project ended up taking me much longer than I intended (for a practice exercise). Since I've done a majority of the processing, I'm going to shelve it for now. I'd love to come back in the future and try to finish this up though. 

## Learning goals

* [x] Practice Python ML workflow
* [x] Learn to build data processing pipelines in Sklearn

## Things learned

* Outlier analysis
* Data cleaning (for exploration) vs. data preprocessing (for modeling)
* Building transformers and Pipelines with Scikit-Learn
* Feature selection using Random Forests
* Apply with anonymous function
* List comprehensions
* Conditional expressions

## Things to for the next project

* Allocate more time to exploration, upfront
* Separate the data cleaning step (exploration) from data processing step (modeling)
* Analyze outliers through scatterplots to decide to remove individual observations

## References

* [Forecasting Movie Box Office Profitability](https://pdfs.semanticscholar.org/6d4f/1003fd164ffe30e2e45dd252715efecf9e61.pdf)
* [ML Data Pipelines with Customs Transformers in Python](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65)
* [Andrew Lukyanenko's Kaggle kernel](https://www.kaggle.com/artgor/eda-feature-engineering-and-model-interpretation)
* [Scikit-Learn Documentation](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html)
* [Feature selection or model tuning: which goes first?](https://stats.stackexchange.com/questions/264533/how-should-feature-selection-and-hyperparameter-optimization-be-ordered-in-the-m)

## Work log

* 1x40 min (preliminary research)
* 1x30 min (July 22)
* 3x30 min (July 23)
* 1x15 min (July 24)
* 4x30 min (July 25)
* 1x30 min, 2x40 min, 1x20 min (July 26)
* 1x30 min (July 27)
* 2x60 min (July 30)
* 4x60 min (Aug 6)
* 2x60 min, 1x30 min (Aug 7)
* 4x60 min (Aug 8)
* 1x60 min, 2x30 min (Aug 9)
* 1x60 min (Aug 14)
* 1x45 min (Aug 15)
* 4x60 min (Aug 16)
* 2x60 min (Aug 17)
* 4x60 min, 1x45 min (Aug 19)
* 2x60 min (Aug 20)
* 6x30 min (Aug 21)
* 3x60 min (Aug 22)
* 1x60 min, 2x90 min (Aug 29)
* Completed Sept 4